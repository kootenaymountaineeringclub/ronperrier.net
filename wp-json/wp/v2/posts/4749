{"id":4749,"date":"2016-03-13T22:52:03","date_gmt":"2016-03-14T05:52:03","guid":{"rendered":"http:\/\/www.ronperrier.net\/?p=4749"},"modified":"2016-03-14T02:12:13","modified_gmt":"2016-03-14T09:12:13","slug":"computer-technology","status":"publish","type":"post","link":"https:\/\/www.ronperrier.net\/2016\/03\/13\/computer-technology\/","title":{"rendered":"COMPUTER TECHNOLOGY"},"content":{"rendered":"<p><strong>AFTER MOORE&#8217;S LAW<\/strong> Double, double, toil and trouble<br \/>\nexcerpted from The Economist March 16 2016<\/p>\n<p>After a glorious 50 years, Moore\u2019s law\u2014which states that computer power doubles every two years at the same cost\u2014is running out of steam. Tim Cross asks what might replace it<br \/>\nIN 1971 a small company called Intel released the 4004, its first ever microprocessor. The chip, measuring 12 square millimetres, contained 2,300 transistors\u2014tiny electrical switches representing the 1s and 0s that are the basic language of computers. The gap between each transistor was 10,000 nanometres (billionths of a metre) in size, about as big as a red blood cell. The result was a miracle of miniaturisation, but still on something close to a human scale. A child with a decent microscope could have counted the individual transistors of the 4004.<br \/>\nThe transistors on the Skylake chips Intel makes today would flummox any such inspection. The chips themselves are ten times the size of the 4004, but at a spacing of just 14 nanometres (nm) their transistors are invisible, for they are far smaller than the wavelengths of light human eyes and microscopes use. If the 4004\u2019s transistors were blown up to the height of a person, the Skylake devices would be the size of an ant.<br \/>\nThe difference between the 4004 and the Skylake is the difference between computer behemoths that occupy whole basements and stylish little slabs 100,000 times more powerful that slip into a pocket. It is the difference between telephone systems operated circuit by circuit with bulky electromechanical switches and an internet that ceaselessly shuttles data packets around the world in their countless trillions. It is a difference that has changed everything from metal-bashing to foreign policy, from the booking of holidays to the designing of H-bombs.<br \/>\nIt is also a difference capable of easy mathematical quantification. In 1965 Gordon Moore, who would later become one of the founders of Intel, a chipmaker, wrote a paper noting that the number of electronic components which could be crammed into an integrated circuit was doubling every year. This exponential increase came to be known as Moore\u2019s law.<br \/>\nIn the 1970s the rate of doubling was reduced to once every two years. Even so, you would have had to be very brave to look at one of Intel\u2019s 4004s in 1971 and believe that such a law would continue to hold for 44 years. After all, double something 22 times and you have 4m times more of it, or perhaps something 4m times better. But that is indeed what has happened. Intel does not publish transistor counts for its Skylake chips, but whereas the 4004 had 2,300 of them, the company\u2019s Xeon Haswell E-5, launched in 2014, sports over 5 billion, just 22 nm apart.<br \/>\nMoore\u2019s law is not a law in the sense of, say, Newton\u2019s laws of motion. But Intel, which has for decades been the leading maker of microprocessors, and the rest of the industry turned it into a self-fulfilling prophecy.<br \/>\nThat fulfilment was made possible largely because transistors have the unusual quality of getting better as they get smaller; a small transistor can be turned on and off with less power and at greater speeds than a larger one. This meant that you could use more and faster transistors without needing more power or generating more waste heat, and thus that chips could get bigger as well as better.<br \/>\nThe components are approaching a fundamental limit of smallness: the atom<br \/>\nMaking chips bigger and transistors smaller was not easy; semiconductor companies have for decades spent heavily on R&#038;D, and the facilities\u2014\u201cfabs\u201d\u2014in which the chips have been made have become much more expensive. But each time transistors shrank, and the chips made out of them became faster and more capable, the market for them grew, allowing the makers to recoup their R&#038;D costs and reinvest in yet more research to make their products still tinier. The demise of this virtuous circle has been predicted many times. \u201cThere\u2019s a law about Moore\u2019s law,\u201d jokes Peter Lee, a vice-president at Microsoft Research: \u201cThe number of people predicting the death of Moore\u2019s law doubles every two years.\u201d But now the computer industry is increasingly aware that the jig will soon be up. For some time, making transistors smaller has no longer been making them more energy-efficient; as a result, the operating speed of high-end chips has been on a plateau since the mid-2000s (see chart). And while the benefits of making things smaller have been decreasing, the costs have been rising. This is in large part because the components are approaching a fundamental limit of smallness: the atom. A Skylake transistor is around 100 atoms across, and the fewer atoms you have, the harder it becomes to store and manipulate electronic 1s and 0s. Smaller transistors now need trickier designs and extra materials. And as chips get harder to make, fabs get ever more expensive. Handel Jones, the CEO of International Business Strategies, reckons that a fab for state-of-the-art microprocessors now costs around $7 billion. He thinks that by the time the industry produces 5nm chips (which at past rates of progress might be in the early 2020s), this could rise to over $16 billion, or nearly a third of Intel\u2019s current annual revenue. In 2015 that revenue, at $55.4 billion, was only 2% more than in 2011. Such slow increases in revenue and big increases in cost seem to point to an obvious conclusion. \u201cFrom an economic standpoint, Moore\u2019s law is over,\u201d says Linley Gwennap, who runs the Linley Group, a firm of Silicon Valley analysts.<br \/>\nThe pace of advance has been slowing for a while. Marc Snir, a supercomputing expert at Argonne National Laboratory, Illinois, points out that the industry\u2019s International Technology Roadmap for Semiconductors, a collaborative document that tries to forecast the near future of chipmaking, has been over-optimistic for a decade. Promised manufacturing innovations have proved more difficult than expected, arriving years late or not at all.<br \/>\nBrian Krzanich, Intel\u2019s boss, has publicly admitted that the firm\u2019s rate of progress has slowed. Intel has a biennial \u201ctick-tock\u201d strategy: in one year it will bring out a chip featuring smaller transistors (\u201ctick\u201d); the following year it tweaks that chip\u2019s design (\u201ctock\u201d) and prepares to shrink the transistors again in the following year. But when its first 14nm chips, codenamed Broadwell, ticked their way to market in 2014 they were nearly a year behind schedule. The tick to 10nm that was meant to follow the tock of the Skylakes has slipped too; Intel has said such products will not now arrive until 2017. Analysts reckon that because of technological problems the company is now on a \u201ctick-tock-tock\u201d cycle. Other big chipmakers have had similar problems.<br \/>\nMoore\u2019s law has not hit a brick wall. Chipmakers are spending billions on new designs and materials that may make transistors amenable to a bit more shrinkage and allow another few turns of the exponential crank. They are also exploring ways in which performance can be improved with customised designs and cleverer programming. In the past the relentless doubling and redoubling of computing power meant there was less of an incentive to experiment with other sorts of improvement.<\/p>\n<p><strong>Try a Different Route<\/strong><br \/>\nMore radically, some hope to redefine the computer itself. One idea is to harness quantum mechanics to perform certain calculations much faster than any classical computer could ever hope to do. Another is to emulate biological brains, which perform impressive feats using very little energy. Yet another is to diffuse computer power rather than concentrating it, spreading the ability to calculate and communicate across an ever greater range of everyday objects in the nascent internet of things. Moore\u2019s law provided an unprecedented combination of blistering progress and certainty about the near future. As that certainty wanes, the effects could be felt far beyond the chipmakers faced with new challenges and costs. In a world where so many things\u2014from the cruising speed of airliners to the median wage\u2014seem to change little from decade to decade, the exponential growth in computing power underlies the future plans of technology providers working on everything from augmented-reality headsets to self-driving cars. More important, it has come to stand in the imagination for progress itself. If something like it cannot be salvaged, the world would look a grimmer place. At the same time, some see benefits in a less predictable world that gives all sorts of new computing technologies an opportunity to come into their own. \u201cThe end of Moore\u2019s law could be an inflection point,\u201d says Microsoft\u2019s Dr Lee. \u201cIt\u2019s full of challenges\u2014but it\u2019s also a chance to strike out in different directions, and to really shake things up.\u201d<\/p>\n<p><strong>More Moore: The incredible Shrinking Transistor<\/strong> &#8211; New sorts of transistors can eke out a few more iterations of Moore\u2019s law, but they will get increasingly expensive<br \/>\nThanks to the exponential power of Moore\u2019s law, the electronic components that run modern computers vastly outnumber all the leaves on the Earth\u2019s trees. Chris Mack, a chipmaking expert, working from a previous estimate by VLSI Research, an analysis firm, reckons that perhaps 400 billion billion (4&#215;1020) transistors were churned out in 2015 alone. That works out at about 13 trillion a second. At the same time they have become unimaginably small: millions could fit on the full stop at the end of this sentence.<br \/>\nA transistor is a sort of switch. To turn it on, a voltage is applied to its gate, which allows the current to flow through the channel between the transistor\u2019s source and drain. When no current flows, the transistor is off. The on-off states represent the 1s and 0s that are the fundamental language of computers.<br \/>\nThe silicon from which these switches are made is a semiconductor, meaning that its electrical properties are halfway between those of a conductor (in which current can flow easily) and an insulator (in which it cannot). The electrical characteristics of a semiconductor can be tweaked, either by a process called \u201cdoping\u201d, in which the material is spiced with atoms of other elements, such as arsenic or boron, or by the application of an electrical field.<br \/>\nIn a silicon transistor, the channel will be doped with one material and the source and drain with another. Doping alters the amount of energy required for any charge to flow through a semiconductor, so where two differently doped materials abut each other, current cannot flow. But when the device is switched on, the electric field from the gate generates a thin, conductive bridge within the channel which completes the circuit, allowing current to flow through.<br \/>\nFor a long time that basic design worked better and better as transistors became ever smaller. But at truly tiny scales it begins to break down. In modern transistors the source and drain are very close together, of the order of 20nm. That causes the channel to leak, with a residual current flowing even when the device is meant to be off, wasting power and generating unwanted heat.<br \/>\nHeat from this and other sources causes serious problems. Many modern chips must either run below their maximum speeds or even periodically switch parts of themselves off to avoid overheating, which limits their performance.<br \/>\nChipmakers are trying various methods to avoid this. One of them, called strained silicon, which was introduced by Intel in 2004, involves stretching the atoms of the silicon crystal further apart than normal, which lubricates the passage of charge carriers through the channel, reducing the heat generated.<br \/>\nIn another technique, first adopted in 2007, metal oxides are used to combat the effects of tunnelling, a quantum phenomenon in which particles (such as electrons) on one side of a seemingly impermeable barrier turn up on the other side without ever passing through the intervening space. Developing more such esoteric techniques may allow chipmakers to go on shrinking transistors for a little longer, but not much.<br \/>\nThe 3D effect Beyond that, two broad changes will be needed. First, the design of the transistor will have to be changed radically. Second, the industry will have to find a replacement for silicon, the electrical properties of which have already been pushed to their limits.<br \/>\nThe design of the transistor will have to be changed radically<br \/>\nOne solution to the problem of leaking current is to redesign the channel and the gate. Conventionally, transistors have been flat, but in 2012 Intel added a third dimension to its products. To enable it to build chips with features just 22nm apart, it switched to transistors known as \u201cfinFET\u201d, which feature a channel that sticks up from the surface of the chip. The gate is then wrapped around the channel\u2019s three exposed sides, which gives it much better control over what takes place inside the channel. These new transistors are trickier to make, but they switch 37% faster than old ones of the same size and consume only half as much power.<br \/>\nThe next logical step, says Mr Snir of Argonne National Laboratory, is \u201cgate-all-around\u201d transistors, in which the channel is surrounded by its gate on all four sides. That offers maximum control, but it adds extra steps to the manufacturing process, since the gate must now be built in multiple sections. Big chipmakers such as Samsung have said that it might take gate-all-around transistors to build chips with features 5nm apart, a stage that Samsung and other makers expect to be reached by the early 2020s.<br \/>\nBeyond that, more exotic solutions may be needed. One idea is to take advantage of the quantum tunnelling that is such an annoyance for conventional transistors, and that will only get worse as transistors shrink further. It is possible, by applying electrical fields, to control the rate at which tunnelling happens. A low rate of leakage would correspond to a 0; a high rate to a 1. The first experimental tunnelling transistor was demonstrated by a team at IBM in 2004. Since then researchers have been working to commercialise them.<br \/>\nIn 2015 a team led by Kaustav Banerjee, of the University of California, reported in Nature that they had built a tunnelling transistor with a working voltage of just 0.1, far below the 0.7V of devices now in use, which means much less heat. But there is more work to be done before tunnelling transistors become viable, says Greg Yeric of ARM, a British designer of microchips: for now they do not yet switch on and off quickly enough to allow them to be used for fast chips. Jim Greer and his colleagues at Ireland\u2019s Tyndall Institute are working on another idea. Their device, called a junctionless nanowire transistor (JNT), aims to help with another problem of building at tiny scales: getting the doping right. \u201cThese days you\u2019re talking about [doping] a very small amount of silicon indeed. You\u2019ll soon be at the point where even one or two misplaced dopant atoms could drastically alter the behaviour of your transistor,\u201d says Dr Greer.<br \/>\nInstead, he and his colleagues propose to build their JNTs, just 3nm across, out of one sort of uniformly doped silicon. Normally that would result in a wire rather than a switch: a device that is uniformly conductive and cannot be turned off. But at these tiny scales the electrical influence of the gate penetrates right through the wire, so the gate alone can prevent current flowing when the transistor is switched off.<br \/>\nWhereas a conventional transistor works by building an electrical bridge between a source and a drain that are otherwise insulated, Dr Greer\u2019s device works the other way: more like a hose in which the gate acts to stop the current from flowing. \u201cThis is true nanotechnology,\u201d he says. \u201cOur device only works at these sorts of scales. The big advantage is you don\u2019t have to worry about manufacturing these fiddly junctions.\u201d<\/p>\n<p><strong>Material Difference<\/strong><br \/>\nChipmakers are also experimenting with materials beyond silicon. Last year a research alliance including Samsung, Global Foundries, IBM and State University New York unveiled a microchip made with components 7nm apart, a technology that is not expected to be in consumers\u2019 hands until 2018 at the earliest. It used the same finFET design as the present generation of chips, with slight modifications, but although most of the device was built from the usual silicon, around half of its transistors had channels made from a <em>silicon-germanium<\/em> (SiGe) alloy.<br \/>\nThis was chosen because it is, in some ways, a better conductor than silicon. Once again, that means lower power usage and allows the transistor to switch on and off more quickly, boosting the speed of the chip. But it is not a panacea, says Heike Riel, the director of the physical-sciences department at IBM Research. Modern chips are built from two types of transistor. One is designed to conduct electrons, which carry a negative charge. The other sort is designed to conduct \u201choles\u201d, which are places in a semiconductor that might contain electrons but happen not to; these, as it turns out, behave as if they were positively charged electrons. And although SiGe excels at transporting holes, it is rather less good at moving electrons than silicon is.<br \/>\nFuture paths to higher performance along these lines will probably require both SiGe and another compound that moves electrons even better than silicon. The materials with the most favourable electrical properties are <em>alloys of elements such as indium, gallium and arsenide<\/em>, collectively known as III-V materials after their location in the periodic table.<br \/>\nThe trouble is that these materials do not mix easily with silicon. The spacing between the atoms in their crystal lattices is different from that in silicon, so adding a layer of them to the silicon substrate from which all chips are made causes stress that can have the effect of cracking the chip.<br \/>\nThe best-known alternative is <em>graphene<\/em>, a single-atom-thick (and hence two-dimensional) form of carbon. Graphene conducts electrons and holes very well. The difficulty is making it stop. Researchers have tried to get around this by doping, squashing or squeezing graphene, or applying electric fields to change its electrical properties. Some progress has been made: the University of Manchester reported a working graphene transistor in 2008; a team led by Guanxiong Liu at the University of California built devices using a property of the material called \u201cnegative resistance\u201d in 2013. But the main impact of graphene, says Dr Yeric, has been to spur interest in other two-dimensional materials. \u201cGraphene sort of unlocked the box,\u201d he says. \u201cNow we\u2019re looking at things like sheets of molybdenum disulphide, or black phosphorous, or phosphorous-boron compounds.\u201d Crucially, all of those, like silicon, can easily be switched on and off.<br \/>\nIf everything goes according to plan, says Dr Yeric, novel transistor designs and new materials might keep things ticking along for another five or six years, by which time the transistors may be 5nm apart. But beyond that \u201cwe\u2019re running out of ways to stave off the need for something really radical.\u201d<br \/>\nHis favoured candidate for that is something called \u201cspintronics\u201d. Whereas electronics uses the charge of an electron to represent information, spintronics uses \u201cspin\u201d, another intrinsic property of electrons that is related to the concept of rotational energy an object possesses. Usefully, spin comes in two varieties, up and down, which can be used to represent 1 and 0. And the computing industry has some experience with spintronics already: it is used in hard drives, for instance.<br \/>\nResearch into spintronic transistors has been going on for more than 15 years, but none has yet made it into production. Appealingly, the voltage needed to drive them is tiny: 10-20 millivolts, hundreds of times lower than for a conventional transistor, which would solve the heat problem at a stroke. But that brings design problems of its own, says Dr Yeric. With such minute voltages, distinguishing a 1 or a 0 from electrical noise becomes tricky.<br \/>\n\u201cIt\u2019s relatively easy to build a fancy new transistor in the lab,\u201d says Linley Gwennap, the analyst. \u201cBut in order to replace what we\u2019re doing today, you need to be able to put billions on a chip, at a reasonable cost, with high reliability and almost no defects. I hate to say never, but it is very difficult.\u201d That makes it all the more important to pursue other ways of making better computers.<\/p>\n<p><strong>New designs: Taking it to another dimension &#8211; How to get more out of existing transistors<\/strong><br \/>\nStrickly speaking, Moore\u2019s law is about the ever greater number of electronic components that can be crammed onto a given device. More generally, though, it is used as shorthand for saying that computers are always getting better. As transistors become harder and harder to shrink, computing firms are starting to look at making better use of the transistors they already have. \u201cManagers in the past wouldn\u2019t want to invest a lot in intensive design,\u201d says Greg Yeric at ARM. \u201cI think that\u2019s going to start shifting.\u201d<br \/>\nOne way is to make the existing chips work harder. Computer chips have a master clock; every time it ticks, the transistors within switch on or off. The faster the clock, the faster the chip can carry out instructions. Increasing clock rates has been the main way of making chips faster over the past 40 years. But since the middle of the past decade clock rates have barely budged.<br \/>\nChipmakers have responded by using the extra transistors that came with shrinking to duplicate a chip\u2019s existing circuitry. Such \u201cmulti-core\u201d chips are, in effect, several processors in one, the idea being that lashing several slower chips together might give better results than relying on a single speedy one. Most modern desktop chips feature four, eight or even 16 cores.<br \/>\nBut, as the industry has discovered, multi-core chips rapidly hit limits. \u201cThe consensus was that if we could keep doing that, if we could go to chips with 1,000 cores, everything would be fine,\u201d says Doug Burger, an expert in chip design at Microsoft. But to get the best out of such chips, programmers have to break down tasks into smaller chunks that can be worked on simultaneously. \u201cIt turns out that\u2019s really hard,\u201d says Dr Burger. Indeed, for some mathematical tasks it is impossible.<br \/>\nAnother approach is to specialise. The most widely used chips, such as Intel\u2019s Core line or those based on ARM\u2019s Cortex design (found in almost every smartphone on the planet) are generalists, which makes them flexible. That comes at a price: they can do a bit of everything but excel at nothing. Tweaking hardware to make it better at dealing with specific mathematical tasks \u201ccan get you something like a 100- to 1,000-fold performance improvement over some general solution\u201d, says Bob Colwell, who helped design Intel\u2019s Pentium chips.<br \/>\nWhen Moore\u2019s law was doubling performance every couple of years at no cost anyway, there was little incentive to customise processing this way. But now that transistors are not necessarily getting faster and cheaper all the time, those tradeoffs are changing.<\/p>\n<p><strong>Something Special<\/strong><br \/>\nThat was Sean Mitchell\u2019s thinking when, a decade ago, he co-founded a company called Movidius. The firm designs chips for use in computer vision, a booming field with applications in everything from robotics to self-driving cars to augmented reality. Movidius has since raised nearly $90m in funding.<br \/>\nDesigning new chips takes years and can cost tens or even hundreds of millions of dollars<br \/>\n\u201cWhen we looked at the general-purpose chips out there,\u201d says Dr Mitchell, \u201cwe found that they were very inefficient.\u201d So Dr Mitchell and his co-founders set about designing their own specialised microprocessor.<br \/>\n\u201cWe\u2019ve got to process high-resolution images, each containing millions of pixels, and coming in at 60, 90 or even 120 frames per second,\u201d he says. By tweaking the hardware to the task at hand\u2014by providing exactly the mix of computational resources necessary for the mathematics of visual processing while leaving out any of the extraneous logic that would allow a general-purpose chip to perform other tasks\u2014Movidius\u2019s Myriad 2 chip can crunch huge amounts of visual information but use less than a watt of power (which is about 20% of the consumption of the chips in smartphones and only about 1% of those in desktop computers). In January the firm announced a deal with Google.<br \/>\nCustom-built chips are already in use in other parts of the computing industry. The best-known examples are the graphics chips used to improve the visuals of video games, designed by firms such as Nvidia and AMD and first marketed to consumers in the mid-1990s. Intel\u2019s newer Pentium chips also come with built-in specialised logic for tasks such as decoding video. But there are downsides.<br \/>\nDesigning new chips takes years and can cost tens or even hundreds of millions of dollars. Specialised chips are also harder to program than general-purpose ones. And, by their very nature, they improve performance only on certain tasks.<br \/>\nA better target for specialised logic, at least at first, might be data centres, the vast computing warehouses that power the servers running the internet.<br \/>\nBecause of the sheer volume of information they process, data centres will always be able to find a use for a chip that can do only one thing, but do it very well.<br \/>\nWith that in mind, Microsoft, one of the world\u2019s biggest software firms and providers of cloud-computing services, is venturing into the chip-design business. In 2014 it announced a new device called Catapult that uses a special kind of chip called a field-programmable gate array (FPGA), the configuration of which can be reshaped at will. FPGAs offer a useful compromise between specialisation and flexibility, says Dr Burger, who led the team that developed Catapult: \u201cThe idea is to have programmable hardware alongside programmable software.\u201d When one task is finished, an FPGA can be reconfigured for another job in less than a second.<br \/>\nThe chips are already in use with Bing, Microsoft\u2019s search engine, and the company says this has doubled the number of queries a server can process in a given time. There are plenty of other potential applications, says Peter Lee, Dr Burger\u2019s boss at Microsoft. FPGAs excel when one specific algorithm has to be applied over and over again to torrents of data. One idea is to use Catapult to encrypt data flowing between computers to keep them secure. Another possibility is to put it to work on voice- and image-recognition jobs for cloud-connected smartphones.<br \/>\nThe technology is not new, but until now there was little reason to use it. What is new is that \u201cthe cloud is growing at an incredible rate,\u201d says Dr Burger. \u201cAnd now that Moore\u2019s law is slowing down, that makes it much harder to add enough computing capacity to keep up. So these sorts of post-Moore projects start to make economic sense.\u201d<br \/>\nAt the IBM research lab on the shores of Lake Zurich, ambitions are set even higher. On a table in one of the labs sits a chip connected by thin hoses to a flask of purple-black liquid. Patrick Ruch, who works in IBM\u2019s Advanced Thermal Packaging group, sees this liquid as the key to a fundamental redesign of data centres. He and his colleagues think they can shrink a modern supercomputer of the sort that occupies a warehouse into a volume about the size of a cardboard box\u2014by making better use of the third dimension.<\/p>\n<p><strong>Brain scan: Bruno Michel<\/strong><br \/>\nIBM\u2019s head of advanced micro-integration reckons biology holds the key to more energy-efficient chips<br \/>\nIN 2011 a supercomputer called Watson, built by IBM, beat two top-rank human champions at \u201cJeopardy!\u201d, an American quiz show. This caused great excitement. Unlike chess, a game of abstract reason and logic, \u201cJeopardy!\u201d is full of puns, double entendres and wordplay; just the sort of thing meant to stump computers. But Bruno Michel, who heads the advanced micro-integration group at IBM\u2019s research lab in Zurich, says it was not a fair fight. \u201cDo you know how much power [Watson] consumes? At the time it was something like 80kW. That\u2019s thousands of times more than the humans it was playing against.\u201d Dr Michel argues that computers are extremely inefficient machines, both in terms of the electricity they consume and the space they take up. A typical desktop machine or a server in a data centre, he reckons, uses only about 0.0001% of its volume to crunch numbers, and perhaps 1% to shuttle the results around. The rest is mostly empty space. The laws of physics set a limit to how efficiently information can be processed, but modern computers perform at only about 0.00004% of that theoretical maximum, he calculates. So for now a big data centre consumes tens of megawatts of power, almost all of which is transformed, literally, into hot air. Moore\u2019s law used to keep a lid on electricity usage even as computing capacity raced ahead because smaller transistors needed less power. That is no longer true. \u201cNowadays the cost of buying a computer or a data centre is less than the cost of running it for a few years,\u201d says Dr Michel. \u201cThat\u2019s a paradigm shift.\u201d Data centres already consume 2% of all the electricity produced in the world. Dr Michel\u2019s benchmark for efficiency is evolution; his original training was in mechanical engineering, but he fell in love with biology after reading a genetics textbook. After earning a PhD in biochemistry and biophysics from the University of Zurich, he joined IBM\u2019s Zurich lab to work with the scanning tunnelling microscope developed there in 1981. It won its inventors a Nobel prize, allowing scientists to see and manipulate individual atoms. That job led to a project on manufacturing technology for flat-screen displays. \u201cI get fascinated by new things, and then I want to start working on them,\u201d he says. \u201cBut my advice is: if you want to work in a new field, don\u2019t be driven by creativity\u2014be driven by impact.\u201d That is how he got involved in his current work on more energy-efficient chips. \u201cIn the middle of the past decade there was a panic in the chip industry\u2014soon we won\u2019t be able to keep these things cool,\u201d he notes. At the same time energy policy was becoming more important as climate change moved up the political agenda. Biology\u2019s secret weapon, he thinks, is the spidery, fractally branching network of blood vessels that supply energy to the brain, allowing most of its volume to be turned over to useful data-processing tasks. As near as neuroscientists can tell, a mammalian brain uses about 70% of its volume for moving information around, 20% for processing it and the remaining 10% to keep everything in the right place and supplied with nutrients. In doing all these things, a human brain consumes about 20 watts of power. That makes it roughly 10,000 times more efficient than the best silicon machines invented by those brains, Dr Michel reckons. One of his favourite charts compares the density and efficiency of brains with a string of computing technologies going back to the second world war. All of them fall on a straight line, suggesting that to match the energy efficiency of the brain, scientists will have to emulate its density. He is now working on a project to build an electronic version of the blood that channels energy to biological brains (see main article). \u201cIt was something like 200 years after the invention of the steam engine before mechanical engineering began to catch up with biology in terms of efficiency,\u201d he says. \u201cIt would be good if computing could accomplish the same thing in half the time.\u201d<\/p>\n<p>Leaving aside innovations like finned transistors (see previous article), modern chips are essentially flat. But a number of companies, including IBM, are now working on stacking chips on top of each other, like flats in a tower block, to allow designers to pack more transistors into a given area. Samsung already sells storage systems made from vertically stacked flash memory. Last year Intel and Micron, a big memory-manufacturer, announced a new memory technology called 3D Xpoint that also uses stacking.<br \/>\nIBM\u2019s researchers are working on something slightly different: chip stacks in which slices of memory are sandwiched between slices of processing logic. That would allow engineers to pack a huge amount of computing into a tiny volume, as well as offering big performance benefits. A traditional computer\u2019s main memory is housed several centimetres from its processor. At silicon speeds, a centimetre is a vast distance. Sending signals across such distances also wastes energy. Moving the memory inside the chip cuts those distances from centimetres to micrometres, allowing it to shuttle data around more quickly. But there are two big problems with 3D chips. The first is heat. Flat chips are bad enough; in a conventional data centre thousands of fans blowing hot air out of the server racks emit a constant roar. As more layers are added, the volume inside the chip, where the heat is generated, grows faster than the outside area from which it can be removed.<br \/>\nThe second problem is getting electricity in. Chips communicate with the outside world via hundreds of metal \u201cpins\u201d on their undersides. Modern chips are so power-hungry that up to 80% of these pins are reserved for transporting electricity, leaving only a few to get data in and out. In 3D those constraints multiply, as the same number of pins must serve a much more complicated chip.<br \/>\nIBM hopes to kill two birds with one stone by fitting its 3D chips with miniscule internal plumbing. Microfluidic channels will carry cooling liquid into the heart of the chip, removing heat from its entire volume at once. The firm has already tested the liquid-cooling technology with conventional, flat chips. The micro fluidic system could ultimately remove around a kilowatt of heat\u2014about the same as the output of one bar of an electric heater\u2014from a cubic centimetre of volume, says Bruno Michel, the head of the group (see Brain scan, previous page).<br \/>\nBut the liquid will do more than cool the chips: it will deliver energy as well. Inspired by his background in biology, Dr Michel has dubbed the liquid \u201celectronic blood\u201d. If he can pull it off, it will do for computer chips what biological blood does for bodies: provide energy and regulate the temperature at the same time. Dr Michel\u2019s idea is a variant of a flow battery, in which power is provided by two liquids that, meeting on either side of a membrane, produce electricity.<br \/>\nFlow batteries are fairly well understood. The electricity industry has been studying them as a way to store intermittent power from renewable energy sources. Dr Michel\u2019s system is still many years away from commercial deployment, but the principle has been established: when Dr Ruch switches on the flow, the chip to which the hoses are connected flickers into life\u2014without a plug or a wire in sight.<\/p>\n<p><strong>Wait For It<\/strong> &#8211; A pipeline of new technologies to prolong Moore\u2019s magic<br \/>\nTHE world\u2019s IT firms spend huge amounts on research and development. In 2015 they occupied three of the top five places in the list of biggest R&#038;D spenders compiled by PricewaterhouseCoopers, a consultancy. Samsung, Intel and Microsoft, the three largest, alone shelled out $37 billion between them. Many of the companies are working on projects to replace the magic of Moore\u2019s law. Here are a few promising ideas.<br \/>\n\u2022\tOptical communication: the use of light instead of electricity to communicate between computers, and even within chips. This should cut energy use and boost performance Hewlett-Packard, Massachusetts Institute of Technology.<br \/>\n\u2022\tBetter memory technologies:building new kinds of fast, dense, cheap memory to ease one bottleneck in computer performanceIntel, Micron.<br \/>\n\u2022\tQuantum-well transistors: the use of quantum phenomena to alter the behaviour of electrical-charge carriers in a transistor to boost its performance, enabling extra iterations of Moore\u2019s law, increased speed and lower power consumptionIntel.<br \/>\n\u2022\tDeveloping new chips and new software to automate the writing of code for machines built from clusters of specialised chips. This has proved especially difficult Soft Machines.<br \/>\n\u2022\tApproximate computing: making computers\u2019 internal representation of numbers less precise to reduce the numbers of bits per calculation and thus save energy; and allowing computers to make random small mistakes in calculations that cancel each other out over time, which will also save energy University of Washington, Microsoft.<br \/>\n\u2022\tNeuromorphic computing: developing devices loosely modelled on the tangled, densely linked bundles of neurons that process information in animal brains. This may cut energy use and prove useful for pattern recognition and other AI-related tasks IBM, Qualcomm.<br \/>\n\u2022\tCarbon nanotube transistors: these rolled-up sheets of graphene promise low power consumption and high speed, as graphene does. Unike graphene, they can also be switched off easily. But they have proved difficult to mass-produce IBM, Stanford University.<\/p>\n<p><strong>Quantum computing: Harnessing weirdness<\/strong><br \/>\nQuantum computers could offer a giant leap in speed\u2014but only for certain applications<br \/>\nTHE D-Wave 2X is a black box, 3.3 metres to a side, that looks a bit like a shorter, squatter version of the enigmatic monoliths from the film \u201c2001: A Space Odyssey\u201d. Its insides, too, are intriguing. Most of the space, says Colin Williams, D-Wave\u2019s director of business development, is given over to a liquid-helium refrigeration system designed to cool it to 0.015 Kelvin, only a shade above the lowest temperature that is physically possible. Magnetic shielding protects the chip at the machine\u2019s heart from ripples and fluctuations in the Earth\u2019s magnetic field.<br \/>\nSuch high-tech coddling is necessary because the D-Wave 2X is no ordinary machine; it is one of the world\u2019s first commercially available quantum computers. In fact, it is not a full-blown computer in the conventional sense of the word, for it is limited to one particular area of mathematics: finding the lowest value of complicated functions. But that specialism can be rather useful, especially in engineering. D-Wave\u2019s client list so far includes Google, NASA and Lockheed Martin, a big American weapons-maker.<br \/>\nD-Wave\u2019s machine has caused controversy, especially among other quantum-computing researchers. For a while academics in the field even questioned that the firm had built a true quantum machine. Those arguments were settled in 2014, in D-Wave\u2019s favour. But it is still not clear whether the machine is indeed faster than its non-quantum rivals.<br \/>\nD-Wave, based in Canada, is only one of many firms in the quantum-computing business. And whereas its machine is highly specialised, academics have also been trying to build more general ones that could attack any problem. In recent years they have been joined by some of the computing industry\u2019s biggest guns, such as Hewlett-Packard, Microsoft, IBM and Google.<br \/>\nQuantum computing is a fundamentally different way of manipulating information. It could offer a huge speed advantage for some mathematical problems that still stump ordinary machines\u2014and would continue to stump them even if Moore\u2019s law were to carry on indefinitely. It is also often misunderstood and sometimes overhyped. That is partly because the field itself is so new that its theoretical underpinnings are still a work in progress. There are some tasks at which quantum machines will be unambiguously faster than the best non-quantum sort. But for a lot of others the advantage is less clear. \u201cIn many cases we don\u2019t know whether a given quantum algorithm will be faster than the best-known classical one,\u201d says Scott Aaronson, a computer scientist at the Massachusetts Institute of Technology. A working quantum computer would be a boon\u2014but no one is sure how much of one.<br \/>\nThe basic unit of classical computing is the bit, the smallest possible chunk of information. A bit can can take just two values: yes or no, on or off, 1 or 0. String enough bits together and you can represent numbers of any size and perform all sorts of mathematical manipulations upon them. But classical machines can deal with just a handful of those bit-strings at a time. And although some of them can now crunch through billions of strings every second, some problems are so complex that even the latest computers cannot keep pace. Finding the prime factors of a big number is one example: the difficulty of the problem increases exponentially as the number in question gets bigger. Each tick of Moore\u2019s law, in other words, enables the factoring of only slightly larger numbers. And finding prime factors forms the mathematical backbone of much of the cryptography that protects data as they scoot around the internet, precisely because it is hard.<br \/>\nQuantum bits, or qubits, behave differently, thanks to two counterintuitive quantum phenomena. The first is \u201csuperposition\u201d, a state of inherent uncertainty that allows particles to exist in a mixture of states at the same time. For instance, a quantum particle, rather than having a specific location, merely has a certain chance of appearing in any one place.<br \/>\nIn computing terms, this means that a qubit, rather than being a certain 1 or a certain 0, exists as a mixture of both. The second quantum phenomenon, \u201centanglement\u201d, binds together the destiny of a quantity of different particles, so that what happens to one of them will immediately affect the others. That allows a quantum computer to manipulate all of its qubits at the same time.<br \/>\nThe upshot is a machine that can represent\u2014and process\u2014vast amounts of data at once. A 300-qubit machine, for instance, could represent 2300 different strings of 1s and 0s at the same time, a number roughly equivalent to the number of atoms in the visible universe. And because the qubits are entangled, it is possible to manipulate all those numbers simultaneously.<br \/>\nYet building qubits is hard. Superpositions are delicate things: the slightest puff of heat, or a passing electromagnetic wave, can cause them to collapse (or \u201cdecohere\u201d), ruining whatever calculation was being run. That is why D-Wave\u2019s machine\u2014and all other quantum computers\u2014have to be so carefully isolated from outside influences. Still, progress has been quick: in 2012 the record for maintaining a quantum superposition without the use of silicon stood at two seconds; by last year it had risen to six hours.<br \/>\nAnother problem is what to build the qubits out of. Academics at the universities of Oxford and Maryland, among others, favour tickling tightly confined ions with laser beams. Hewlett-Packard, building on its expertise in optics, thinks that photons\u2014the fundamental particles of light\u2014hold the key. Microsoft is pursuing a technology that is exotic even by the standards of quantum computing, involving quasi-particles called anyons. Like those \u201choles\u201d in a semiconductor, anyons are not real particles, but a mathematically useful way of describing phenomena that behave as if they were. Microsoft is currently far behind any of its competitors, but hopes eventually to come up with more elegantly designed and much less error-prone machines than the rest.<br \/>\nProbably the leading approach, used by Google, D-Wave and IBM, is to represent qubits as currents flowing through superconducting wires (which offer no electrical resistance at all). The presence or absence of a current\u2014or alternatively, whether it is circulating clockwise or anti-clockwise\u2014stands for a 1 or a 0. What makes this attractive is that the required circuits are relatively easy to etch into silicon, using manufacturing techniques with which the industry is already familar. And superconducting circuits are becoming more robust, too.<br \/>\nLast year a team led by John Martinis, a quantum physicist working at Google, published a paper describing a system of nine superconducting qubits in which four could be examined without collapsing the other five, allowing the researchers to check for, and correct, mistakes. \u201cWe\u2019re finally getting to the stage now where we can start to build an entire system,\u201d says Dr Martinis.<br \/>\nA quantum computer can represent\u2014and process\u2014vast amounts of data at once<br \/>\nUsing a quantum computer is hard, too. In order to get the computer to answer the question put to it, its operator must measure the state of its qubits. That causes them to collapse out of their superposed state so that the result of the calculation can be read. And if the measurement is done the wrong way, the computer will spit out just one of its zillions of possible states, and almost certainly the wrong one. \u201cYou will have built the world\u2019s most expensive random-number generator,\u201d says Dr Aaronson.<br \/>\nFor a quantum algorithm to work, the machine must be manipulated in such a way that the probability of obtaining the right answer is continually reinforced while the chances of getting a wrong answer are suppressed. One of the first useful algorithms for this purpose was published in 1994 by Peter Shor, a mathematician; it is designed to solve the prime-factorising problem explained above. Dr Aaronson points out that alongside error correction of the sort that Dr Martinis has pioneered, Dr Shor\u2019s algorithm was one of the crucial advances which persuaded researchers that quantum computers were more than just a theoretical curiosity. Since then more such algorithms have been discovered. Some are known to be faster than their best-known classical rivals; others have yet to prove their speed advantage.<\/p>\n<p><strong>A Cryptographer\u2019s Dream<\/strong><br \/>\nThat leaves open the question of what, exactly, a quantum computer would be good for. Matthias Troyer, of the Swiss Federal Institute of Technology in Zurich, has spent the past four years conducting a rigorous search for a \u201ckiller app\u201d for quantum computers. One commonly cited, and exciting, application is in codebreaking. Dr Shor\u2019s algorithm would allow a quantum computer to make short work of most modern cryptographic codes. Documents leaked in early 2014 by Edward Snowden, a former American spy, proved what cryptographers had long suspected: that America\u2019s National Security Agency (NSA) was working on quantum computers for exactly that reason. Last August the NSA recommended that the American government begin switching to new codes that are potentially less susceptible to quantum attack. The hope is that this will pre-empt any damage before a working quantum computer is built.<br \/>\nAnother potential killer app is artificial intelligence (AI). Firms such as Google, Facebook and Baidu, China\u2019s biggest search engine, are already putting significant sums into computers than can teach themselves to understand human voices, identify objects in images, interpret medical scans and so on. Such AI programs must be trained before they can be deployed. For a face-recognition algorithm, for instance, that means showing it thousands of images. The computer has to learn which of these are faces and which are not, and perhaps which picture shows a specific face and which not, and come up with a rule that efficiently transforms the input of an image into a correct identification.<br \/>\nOrdinary computers can already perform all these tasks, but D-Wave\u2019s machine is meant to be much faster. In 2013 Google and NASA put one of them into their newly established Quantum AI Lab to see whether the machine could provide a speed boost. The practical value of this would be immense, but Dr Troyer says the answer is not yet clear.<br \/>\nIn his view, the best use for quantum computers could be in simulating quantum mechanics itself, specifically the complicated dance of electrons that is chemistry. With conventional computers, that is fiendishly difficult. The 2013 Nobel prize for chemistry was awarded for the development of simplified models that can be run on classical computers. But, says Dr Troyer, \u201cfor complex molecules, the existing [models] are not good enough.\u201d His team reckoned that a mixed approach, combining classical machines with the quantum sort, would do better. Their first efforts yielded algorithms with running times of hundreds of years. Over the past three years, though, the researchers have refined their algorithms to the point where a simulation could be run in hundreds of seconds instead.<br \/>\nIt may not be as exciting as AI or code-breaking, but being able to simulate quantum processes accurately could revolutionise all sorts of industrial chemistry. The potential applications Dr Troyer lists include better catalysts, improved engine design, a better understanding of biological molecules and improving things like the Haber process, which produces the bulk of the world\u2019s fertilisers. All of those are worthwhile goals that no amount of conventional computing power seems likely to achieve.<\/p>\n<p><strong>What Comes Next: Horses for Courses<\/strong> &#8211; The end of Moore\u2019s law will make the computer industry a much more complicated place<br \/>\nWHEN Moore\u2019s law was in its pomp, life was simple. Computers got better in predictable ways and at a predictable rate. As the metronome begins to falter, the computer industry will become a more complicated place. Things like clever design and cunning programming are useful, says Bob Colwell, the Pentium chip designer, \u201cbut a collection of one-off ideas can\u2019t make up for the lack of an underlying exponential.\u201d<br \/>\nProgress will become less predictable, narrower and less rapid than the industry has been used to. \u201cAs Moore\u2019s law slows down, we are being forced to make tough choices between the three key metrics of power, performance and cost,\u201d says Greg Yeric, the chip designer at ARM. \u201cNot all end uses will be best served by one particular answer.\u201d<br \/>\nAnd as computers become ever more integrated into everyday life, the definition of progress will change. \u201cRemember: computer firms are not, fundamentally, in it to make ever smaller transistors,\u201d says Marc Snir, of Argonne National Laboratory. \u201cThey\u2019re in it to produce useful products, and to make money.\u201d<br \/>\nMoore\u2019s law has moved computers from entire basements to desks to laps and hence to pockets. The industry is hoping that they will now carry on to everything from clothes to smart homes to self-driving cars. Many of those applications demand things other than raw performance. \u201cI think we will see a lot of creativity unleashed over next decade,\u201d says Linley Gwennap, the Silicon Valley analyst. \u201cWe\u2019ll see performance improved in different ways, and existing tech used in new ways.\u201d<br \/>\nMr Gwennap points to the smartphone as an example of the kind of innovation that might serve as a model for the computing industry. Only four years after the iPhone first launched, in 2011, smartphone sales outstripped those of conventional PCs. Smartphones would never have been possible without Moore\u2019s law. But although the small, powerful, frugal chips at their hearts are necessary, they are not sufficient. The appeal of smartphones lies not just in their performance but in their light, thin and rugged design and their modest power consumption. To achieve this, Apple has been heavily involved in the design of the iPhone\u2019s chips.<br \/>\nAnd they do more than crunch numbers. Besides their microprocessors, smartphones contain tiny versions of other components such as accelerometers, GPS receivers, radios and cameras. That combination of computing power, portability and sensor capacity allows smartphones to interact with the world and with their users in ways that no desktop computer ever could.<br \/>\nVirtual reality (VR) is another example. This year the computer industry will make another attempt at getting this off the ground, after a previous effort in the 1990s. Firms such as Oculus, an American startup bought by Facebook, Sony, which manufactures the PlayStation console, and HTC, a Taiwanese electronics firm, all plan to launch virtual-reality headsets to revolutionise everything from films and video games to architecture and engineering.<br \/>\nA certain amount of computing power is necessary to produce convincing graphics for VR users, but users will settle for far less than photo-realism. The most important thing, say the manufacturers, is to build fast, accurate sensors that can keep track of where a user\u2019s head is pointing, so that the picture shown by the goggles can be updated correctly. If the sensors are inaccurate, the user will feel \u201cVR sickness\u201d, an unpleasant sensation closely related to motion sickness. But good sensors do not require superfast chips.<br \/>\nThe biggest market of all is expected to be the \u201cinternet of things\u201d\u2014in which cheap chips and sensors will be attached to everything, from fridges that order food or washing machines that ask clothes for laundering instructions to paving slabs in cities to monitor traffic or pollution. Gartner, a computing consultancy, reckons that by 2020 the number of connected devices in the world could run to 21 billion.<\/p>\n<p><strong>Never Mind the Quality, Feel the Bulk<\/strong><br \/>\nThe processors needed to make the internet of things happen will need to be as cheap as possible, says Dr Yeric. They will have to be highly energy-efficient, and ideally able to dispense with batteries, harvesting energy from their surroundings, perhaps in the form of vibrations or ambient electromagnetic waves. They will need to be able to communicate, both with each other and with the internet at large, using tiny amounts of power and in an extremely crowded radio spectrum. What they will not need is the latest high-tech specification. \u201cI suspect most of the chips that power the internet of things will be built on much older, cheaper production lines,\u201d says Dr Yeric.<br \/>\nChurning out untold numbers of low-cost chips to turn dumb objects into smart ones will be a big, if unglamorous, business. At the same time, though, the vast amount of data thrown off by the internet of things will boost demand for the sort of cutting-edge chips that firms such as Intel specialise in. According to Dr Yeric, \u201cif we really do get sensors everywhere, you could see a single engineering company\u2014say Rolls Royce [a British manufacturer of turbines and jet engines]\u2014having to deal with more data than the whole of YouTube does today.\u201d<br \/>\nIncreasingly, though, those chips will sit not in desktops but in the data centres that make up the rapidly growing computing \u201ccloud\u201d. The firms involved keep their financial cards very close to their chests, but making those high-spec processors is Intel\u2019s most profitable business. Goldman Sachs, a big investment bank, reckons that cloud computing grew by 30% last year and will keep on expanding at that rate at least until 2018.<br \/>\nThe scramble for that market could upend the industry\u2019s familiar structure. Big companies that crunch a lot of numbers, such as Facebook and Amazon, already design their own data centres, but they buy most of their hardware off the shelf from firms such as Intel and Cisco, which makes routers and networking equipment. Microsoft, a software giant, has started designing chips of its own. Given the rapid growth in the size of the market for cloud computing, other software firms may soon follow.<br \/>\nThe twilight of Moore\u2019s law, then, will bring change, disorder and plenty of creative destruction. An industry that used to rely on steady improvements in a handful of devices will splinter. Software firms may begin to dabble in hardware; hardware makers will have to tailor their offerings more closely to their customers\u2019 increasingly diverse needs. But, says Dr Colwell, remember that consumers do not care about Moore\u2019s law per se: \u201cMost of the people who buy computers don\u2019t even know what a transistor does.\u201d They simply want the products they buy to keep getting ever better and more useful. In the past, that meant mostly going for exponential growth in speed. That road is beginning to run out. But there will still be plenty of other ways to make better computers.<\/p>\n","protected":false},"excerpt":{"rendered":"<p>AFTER MOORE&#8217;S LAW Double, double, toil and trouble excerpted from The Economist March 16 2016 After a glorious 50 years, Moore\u2019s law\u2014which states that computer power doubles every two years at the same cost\u2014is running out of steam. Tim Cross &hellip; <a href=\"https:\/\/www.ronperrier.net\/2016\/03\/13\/computer-technology\/\">Continue reading <span class=\"meta-nav\">&rarr;<\/span><\/a><\/p>\n","protected":false},"author":1,"featured_media":0,"comment_status":"closed","ping_status":"closed","sticky":false,"template":"","format":"standard","meta":{"jetpack_post_was_ever_published":false,"_jetpack_newsletter_access":"","_jetpack_dont_email_post_to_subs":false,"_jetpack_newsletter_tier_id":0,"_jetpack_memberships_contains_paywalled_content":false,"_jetpack_memberships_contains_paid_content":false,"footnotes":"","jetpack_publicize_message":"","jetpack_publicize_feature_enabled":true,"jetpack_social_post_already_shared":true,"jetpack_social_options":{"image_generator_settings":{"template":"highway","default_image_id":0,"enabled":false},"version":2}},"categories":[1],"tags":[],"class_list":["post-4749","post","type-post","status-publish","format-standard","hentry","category-uncategorized"],"aioseo_notices":[],"jetpack_publicize_connections":[],"jetpack_featured_media_url":"","jetpack_sharing_enabled":true,"jetpack_shortlink":"https:\/\/wp.me\/p2Ncip-1eB","_links":{"self":[{"href":"https:\/\/www.ronperrier.net\/wp-json\/wp\/v2\/posts\/4749","targetHints":{"allow":["GET"]}}],"collection":[{"href":"https:\/\/www.ronperrier.net\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/www.ronperrier.net\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/www.ronperrier.net\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/www.ronperrier.net\/wp-json\/wp\/v2\/comments?post=4749"}],"version-history":[{"count":0,"href":"https:\/\/www.ronperrier.net\/wp-json\/wp\/v2\/posts\/4749\/revisions"}],"wp:attachment":[{"href":"https:\/\/www.ronperrier.net\/wp-json\/wp\/v2\/media?parent=4749"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/www.ronperrier.net\/wp-json\/wp\/v2\/categories?post=4749"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/www.ronperrier.net\/wp-json\/wp\/v2\/tags?post=4749"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}