{"id":12458,"date":"2020-01-05T21:40:25","date_gmt":"2020-01-06T05:40:25","guid":{"rendered":"http:\/\/www.ronperrier.net\/?p=12458"},"modified":"2020-01-05T21:40:25","modified_gmt":"2020-01-06T05:40:25","slug":"your-lying-mind","status":"publish","type":"post","link":"https:\/\/www.ronperrier.net\/2020\/01\/05\/your-lying-mind\/","title":{"rendered":"YOUR LYING MIND"},"content":{"rendered":"<p><strong>The Cognitive Biases Tricking Your Brain<br \/>\n<\/strong>Science suggests we\u2019re hardwired to delude ourselves. Can we do anything about it?<\/p>\n<p>By\u00a0Ben Yagoda<br \/>\nAtlantic September 2019<\/p>\n<p>I am staring\u00a0at a photograph of myself that shows me 20 years older than I am now. I have not stepped into the twilight zone. Rather, I am trying to rid myself of some measure of my present bias, which is the tendency people have, when considering a trade-off between two future moments, to more heavily weight the one closer to the present. A great many academic studies have shown this bias\u2014also known as hyperbolic discounting\u2014to be robust and persistent.<\/p>\n<p>Most of them have focused on money. When asked whether they would prefer to have, say, $150 today or $180 in one month, people tend to choose the $150. Giving up a 20 percent return on investment is a bad move\u2014which is easy to recognize when the question is thrust away from the present. Asked whether they would take $150 a year from now or $180 in 13 months, people are overwhelmingly willing to wait an extra month for the extra $30.<\/p>\n<p>Present bias shows up not just in experiments, of course, but in the real world. Especially in the United States,\u00a0people egregiously undersave for retirement\u2014even when they make enough money to not spend their whole paycheck on expenses, and even when they work for a company that will kick in additional funds to retirement plans when they contribute.<\/p>\n<p>That state of affairs led a scholar named Hal Hershfield to play around with photographs. Hershfield is a marketing professor at UCLA whose research starts from the idea that people are \u201cestranged\u201d from their future self. As a result, he explained in a 2011 paper, \u201csaving is like a choice between spending money today or giving it to a stranger years from now.\u201d The paper\u00a0described an attempt\u00a0by Hershfield and several colleagues to modify that state of mind in their students. They had the students observe, for a minute or so, virtual-reality avatars showing what they would look like at age 70. Then they asked the students what they would do if they unexpectedly came into $1,000. The students who had looked their older self in the eye said they would put an average of $172 into a retirement account. That\u2019s more than double the amount that would have been invested by members of the control group, who were willing to sock away an average of only $80.<\/p>\n<p>I am already old\u2014in my early 60s, if you must know\u2014so Hershfield furnished me not only with an image of myself in my 80s (complete with age spots, an exorbitantly asymmetrical face, and wrinkles as deep as a Manhattan pothole) but also with an image of my daughter as she\u2019ll look decades from now. What this did, he explained, was make me ask myself,\u00a0How will I feel toward the end of my life if my offspring are not taken care of?<\/p>\n<p>When people hear\u00a0the word\u00a0bias, many if not most will think of either racial prejudice or news organizations that slant their coverage to favor one political position over another. Present bias, by contrast, is an example of cognitive bias\u2014the collection of faulty ways of thinking that is apparently hardwired into the human brain. The collection is large. Wikipedia\u2019s \u201cList of cognitive biases\u201d contains 185 entries, from <strong>actor-observer bias<\/strong> (\u201cthe tendency for explanations of other individuals\u2019 behaviors to overemphasize the influence of their personality and underemphasize the influence of their situation \u2026 and for explanations of one\u2019s own behaviors to do the opposite\u201d) to the <strong>Zeigarnik effect<\/strong> (\u201cuncompleted or interrupted tasks are remembered better than completed ones\u201d).<\/p>\n<p>Some of the 185 are dubious or trivial. The\u00a0ikea\u00a0effect, for instance, is defined as \u201cthe tendency for people to place a disproportionately high value on objects that they partially assembled themselves.\u201d And others closely resemble one another to the point of redundancy. But a solid group of 100 or so biases has been repeatedly shown to exist, and can make a hash of our lives.<\/p>\n<p>The <strong>gambler\u2019s fallacy<\/strong> makes us absolutely certain that, if a coin has landed heads up five times in a row, it\u2019s more likely to land tails up the sixth time. In fact, the odds are still 50-50. Optimism bias leads us to consistently underestimate the costs and the duration of basically every project we undertake. <strong>Availability bias<\/strong> makes us think that, say, traveling by plane is more dangerous than traveling by car. (Images of plane crashes are more vivid and dramatic in our memory and imagination, and hence more available to our consciousness.)<\/p>\n<p>The <strong>anchoring effect<\/strong> is our tendency to rely too heavily on the first piece of information offered, particularly if that information is presented in numeric form, when making decisions, estimates, or predictions. This is the reason negotiators start with a number that is deliberately too low or too high: They know that number will \u201canchor\u201d the subsequent dealings. A striking illustration of anchoring is\u00a0an experiment\u00a0in which participants observed a roulette-style wheel that stopped on either 10 or 65, then were asked to guess what percentage of United Nations countries is African. The ones who saw the wheel stop on 10 guessed 25 percent, on average; the ones who saw the wheel stop on 65 guessed 45 percent. (The correct percentage at the time of the experiment was about 28 percent.)<\/p>\n<p>The effects of biases do not play out just on an individual level. Last year, President Donald Trump decided to send more troops to Afghanistan, and thereby walked right into the <strong>sunk-cost fallacy<\/strong>.\u00a0He said, \u201cOur nation must seek an honorable and enduring outcome worthy of the tremendous sacrifices that have been made, especially the sacrifices of lives.\u201d Sunk-cost thinking tells us to stick with a bad investment because of the money we have already lost on it; to finish an unappetizing restaurant meal because, after all, we\u2019re paying for it; to prosecute an unwinnable war because of the investment of blood and treasure. In all cases, this way of thinking is rubbish.<\/p>\n<p>\u201cWe would all like to have a warning bell that rings loudly whenever we are about to make a serious error,\u201d Kahneman writes, \u201cbut no such bell is available.\u201d<\/p>\n<p>If I had to single out a particular bias as the most pervasive and damaging, it would probably be\u00a0<strong>confirmation bias<\/strong>. That\u2019s the effect that leads us to look for evidence confirming what we already think or suspect, to view facts and ideas we encounter as further confirmation, and to discount or ignore any piece of evidence that seems to support an alternate view. Confirmation bias shows up most blatantly in our current political divide, where each side seems unable to allow that the other side is right about anything.<br \/>\nConfirmation bias plays out in lots of other circumstances, sometimes with terrible consequences. To quote the\u00a02005 report to the president\u00a0on the lead-up to the Iraq War: \u201cWhen confronted with evidence that indicated Iraq did not have [weapons of mass destruction], analysts tended to discount such information. Rather than weighing the evidence independently, analysts accepted information that fit the prevailing theory and rejected information that contradicted it.\u201d<\/p>\n<p>The whole idea\u00a0of cognitive biases and faulty heuristics\u2014the shortcuts and rules of thumb by which we make judgments and predictions\u2014was more or less invented in the 1970s by Amos Tversky and Daniel Kahneman, social scientists who started their careers in Israel and eventually moved to the United States. They were the researchers who conducted the African-countries-in-the-UN experiment. Tversky died in 1996. Kahneman won the 2002 Nobel Prize in Economics for the work the two men did together, which he summarized in his 2011 best seller,\u00a0Thinking, Fast and Slow. Another best seller, last year\u2019s\u00a0The Undoing Project, by Michael Lewis, tells the story of the\u00a0sometimes contentious\u00a0collaboration between Tversky and Kahneman. Lewis\u2019s earlier book\u00a0Moneyball\u00a0was really about how his hero, the baseball executive Billy Beane, countered the cognitive biases of old-school scouts\u2014notably fundamental <strong>attribution error<\/strong>, whereby, when assessing someone\u2019s behavior, we put too much weight on his or her personal attributes and too little on external factors, many of which can be measured with statistics.<\/p>\n<p>Another key figure in the field is the University of Chicago economist Richard Thaler. One of the biases he\u2019s most linked with is the\u00a0<strong>endowment effect<\/strong>, which leads us to place an irrationally high value on our possessions. In an experiment conducted by Thaler, Kahneman, and Jack L. Knetsch, half the participants were given a mug and then asked how much they would sell it for. The average answer was $5.78. The rest of the group said they would spend, on average, $2.21 for the same mug. This flew in the face of classic economic theory, which says that at a given time and among a certain population, an item has a market value that does\u00a0not\u00a0depend on whether one owns it or not. Thaler won the 2017 Nobel Prize in Economics.<\/p>\n<p>Most books and articles about cognitive bias contain a brief passage, typically toward the end, similar to this one in\u00a0Thinking, Fast and Slow: \u201cThe question that is most often asked about cognitive illusions is whether they can be overcome. The message \u2026 is not encouraging.\u201d<\/p>\n<p>Kahneman and others draw an analogy based on an understanding of <strong>the\u00a0M\u00fcller-Lyer illusion<\/strong>, two parallel lines with arrows at each end. One line\u2019s arrows point in; the other line\u2019s arrows point out. Because of the direction of the arrows, the latter line appears shorter than the former, but in fact the two lines are the same length. Here\u2019s the key: Even after we have measured the lines and found them to be equal, and have had the neurological basis of the illusion explained to us, we still perceive one line to be shorter than the other.<\/p>\n<p>At least with the optical illusion, our slow-thinking, analytic mind\u2014what Kahneman calls System 2\u2014will recognize a M\u00fcller-Lyer situation and convince itself not to trust the fast-twitch System 1\u2019s perception. But that\u2019s not so easy in the real world, when we\u2019re dealing with people and situations rather than lines. \u201cUnfortunately, this sensible procedure is least likely to be applied when it is needed most,\u201d Kahneman writes. \u201cWe would all like to have a warning bell that rings loudly whenever we are about to make a serious error, but no such bell is available.\u201d<\/p>\n<p>Because biases appear to be so hardwired and inalterable, most of the attention paid to countering them hasn\u2019t dealt with the problematic thoughts, judgments, or predictions themselves. Instead, it has been devoted to changing\u00a0behavior, in the form of incentives or \u201cnudges.\u201d For example, while present bias has so far proved intractable, employers have been able to nudge employees into contributing to retirement plans by making saving the default option; you have to actively take steps in order to\u00a0not\u00a0participate. That is, laziness or inertia can be more powerful than bias. Procedures can also be organized in a way that dissuades or prevents people from acting on biased thoughts. A well-known example: the checklists for doctors and nurses put forward by Atul Gawande in his book\u00a0The Checklist Manifesto.<\/p>\n<p>Is it really impossible, however, to shed or significantly mitigate one\u2019s biases? Some studies have tentatively answered that question in the affirmative. These experiments are based on the reactions and responses of randomly chosen subjects, many of them college undergraduates: people, that is, who care about the $20 they are being paid to participate, not about modifying or even learning about their behavior and thinking. But what if the person undergoing the de-biasing strategies was highly motivated and self-selected? In other words, what if it was me?<\/p>\n<p>Naturally, I wrote to Daniel Kahneman, who at 84 still holds an appointment at the Woodrow Wilson School of Public and International Affairs, at Princeton, but spends most of his time in Manhattan. He answered swiftly and agreed to meet. \u201cI should,\u201d he said, \u201cat least try to talk you out of your project.\u201d<br \/>\nI met with Kahneman at a Le Pain Quotidien in Lower Manhattan. He is tall, soft-spoken, and affable, with a pronounced accent and a wry smile. Over an apple pastry and tea with milk, he told me, \u201cTemperament has a lot to do with my position. You won\u2019t find anyone more pessimistic than I am.\u201d<br \/>\nIn this context, his pessimism relates, first, to the impossibility of effecting any changes to System 1\u2014the quick-thinking part of our brain and the one that makes mistaken judgments tantamount to the M\u00fcller-Lyer line illusion. \u201cI see the picture as unequal lines,\u201d he said. \u201cThe goal is not to trust what I think I see. To understand that I shouldn\u2019t believe my lying eyes.\u201d That\u2019s doable with the optical illusion, he said, but extremely difficult with real-world cognitive biases.<br \/>\nThe most effective check against them, as Kahneman says, is from the outside: Others can perceive our errors more readily than we can. And \u201cslow-thinking organizations,\u201d as he puts it, can institute policies that include the monitoring of individual decisions and predictions. They can also require procedures such as checklists and \u201cpremortems,\u201d an idea and term thought up by Gary Klein, a cognitive psychologist. A premortem attempts to counter optimism bias by requiring team members to imagine that a project has gone very, very badly and write a sentence or two describing how that happened. Conducting this exercise, it turns out, helps people think ahead.<br \/>\n\u201cMy position is that none of these things have any effect on System 1,\u201d Kahneman said. \u201cYou can\u2019t improve intuition. Perhaps, with very long-term training, lots of talk, and exposure to behavioral economics, what you can do is\u00a0cue\u00a0reasoning, so you can engage System 2 to follow rules. Unfortunately, the world doesn\u2019t provide cues. And for most people, in the heat of argument the rules go out the window.<br \/>\n\u201cThat\u2019s my story. I really hope I don\u2019t have to stick to it.\u201d<\/p>\n<p>As it happened,\u00a0right around the same time I was communicating and meeting with Kahneman, he was exchanging emails with Richard E. Nisbett, a social psychologist at the University of Michigan. The two men had been professionally connected for decades. Nisbett was instrumental in disseminating Kahneman and Tversky\u2019s work, in a 1980 book called\u00a0Human Inference: Strategies and Shortcomings of Social Judgment. And in\u00a0Thinking, Fast and Slow, Kahneman describes an even earlier Nisbett article that showed subjects\u2019 disinclination to believe statistical and other general evidence, basing their judgments instead on individual examples and vivid anecdotes. (This bias is known as <strong>base-rate neglect<\/strong>.)<\/p>\n<p>But over the years, Nisbett had come to emphasize in his research and thinking the possibility of training people to overcome or avoid a number of pitfalls, including base-rate neglect, fundamental attribution error, and the sunk-cost fallacy. He had emailed Kahneman in part because he had been working on a memoir, and wanted to discuss a conversation he\u2019d had with Kahneman and Tversky at a long-ago conference. Nisbett had the distinct impression that Kahneman and Tversky had been angry\u2014that they\u2019d thought what he had been saying and doing was an implicit criticism of them. Kahneman recalled the interaction, emailing back: \u201cYes, I remember we were (somewhat) annoyed by your work on the ease of training statistical intuitions (angry is much too strong).\u201d<br \/>\nWhen Nisbett has to give an example of his approach, he usually brings up the <strong>baseball-phenom<\/strong> survey. This involved telephoning University of Michigan students on the pretense of conducting a poll about sports, and asking them why there are always several Major League batters with .450 batting averages early in a season, yet no player has ever finished a season with an average that high. When he talks with students who haven\u2019t taken Introduction to Statistics, roughly half give erroneous reasons such as \u201cthe pitchers get used to the batters,\u201d \u201cthe batters get tired as the season wears on,\u201d and so on. And about half give the right answer: the law of large numbers, which holds that outlier results are much more frequent when the sample size (at bats, in this case) is small. Over the course of the season, as the number of at bats increases, regression to the mean is inevitable. When Nisbett asks the same question of students who have completed the statistics course, about 70 percent give the right answer. He believes this result shows, pace Kahneman, that the law of large numbers can be absorbed into System 2\u2014and maybe into System 1 as well, even when there are minimal cues.<br \/>\nNisbett\u2019s second-favorite example is that economists, who have absorbed the lessons of the sunk-cost fallacy,\u00a0routinely walk out\u00a0of bad movies and\u00a0leave bad restaurant meals uneaten.<br \/>\nI spoke with Nisbett by phone and asked him about his disagreement with Kahneman. He still sounded a bit uncertain. \u201cDanny seemed to be convinced that what I was showing was trivial,\u201d he said. \u201cTo him it was clear: Training was hopeless for all kinds of judgments. But we\u2019ve tested Michigan students over four years, and they show a huge increase in ability to solve problems. Graduate students in psychology also show a huge gain.\u201d<br \/>\nNisbett writes in his 2015 book,\u00a0Mindware: Tools for Smart Thinking, \u201cI know from my own research on teaching people how to reason statistically that just a few examples in two or three domains are sufficient to improve people\u2019s reasoning for an indefinitely large number of events.\u201d<\/p>\n<p>In one of his emails to Nisbett, Kahneman had suggested that the difference between them was to a significant extent a result of temperament: pessimist versus optimist. In a response, Nisbett suggested another factor: \u201cYou and Amos specialized in hard problems for which you were drawn to the wrong answer. I began to study easy problems, which you guys would never get wrong but untutored people routinely do \u2026 Then you can look at the effects of instruction on such easy problems, which turn out to be huge.\u201d<\/p>\n<p>An example of an easy problem is the .450 hitter early in a baseball season. An example of a hard one is \u201cthe <strong>Linda problem<\/strong>,\u201d which was the basis of one of Kahneman and Tversky\u2019s early articles. Simplified, the experiment presented subjects with the characteristics of a fictional woman, \u201cLinda,\u201d including her commitment to social justice, college major in philosophy, participation in antinuclear demonstrations, and so on. Then the subjects were asked which was more likely: (a) that Linda was a bank teller, or (b) that she was a bank teller and active in the feminist movement. The correct answer is (a), because it is always more likely that one condition will be satisfied in a situation than that the condition\u00a0plus\u00a0a second one will be satisfied. But because of the <strong>conjunction fallacy<\/strong> (the assumption that multiple specific conditions are more probable than a single general one) and the representativeness heuristic (our strong desire to apply stereotypes), more than 80 percent of undergraduates surveyed answered (b).<br \/>\nNisbett justifiably asks how often in real life we need to make a judgment like the one called for in the Linda problem. I cannot think of any applicable scenarios in my life. It is a bit of a logical parlor trick.<\/p>\n<p>Nisbett suggested that\u00a0I take \u201cMindware: Critical Thinking for the Information Age,\u201d an online Coursera course in which he goes over what he considers the most effective de-biasing skills and concepts. Then, to see how much I had learned, I would take a survey he gives to Michigan undergraduates. So I did.<br \/>\nThe course consists of eight lessons by Nisbett\u2014who comes across on-screen as the authoritative but approachable psych professor we all would like to have had\u2014interspersed with some graphics and quizzes. I recommend it. He explains the availability heuristic this way: \u201cPeople are surprised that suicides outnumber homicides, and drownings outnumber deaths by fire. People always think crime is increasing\u201d even if it\u2019s not.<br \/>\nHe addresses the\u00a0logical fallacy of confirmation bias, explaining that people\u2019s tendency, when testing a hypothesis they\u2019re inclined to believe, is to seek examples confirming it. But Nisbett points out that no matter how many such examples we gather, we can never prove the proposition. The right thing to do is to look for cases that would disprove it.<br \/>\nAnd he approaches base-rate neglect by means of his own strategy for choosing which movies to see. His decision is never dependent on ads, or a particular review, or whether a film sounds like something he would enjoy. Instead, he says, \u201cI live by base rates. I don\u2019t read a book or see a movie unless it\u2019s highly recommended by people I trust.<br \/>\n\u201cMost people think they\u2019re not like other people. But they are.\u201d<\/p>\n<p>When I finished the course, Nisbett sent me the survey he and colleagues administer to Michigan undergrads. It contains a few dozen problems meant to measure the subjects\u2019 resistance to cognitive biases. For example:<br \/>\nBecause of confirmation bias, many people who haven\u2019t been trained answer (e). But the correct answer is (c). The only thing you can hope to do in this situation is\u00a0disprove\u00a0the rule, and the only way to do that is to turn over the cards displaying the letter A (the rule is disproved if a number other than 4 is on the other side) and the number 7 (the rule is disproved if an A is on the other side).<br \/>\nI got it right. Indeed, when I emailed my completed test, Nisbett replied, \u201cMy guess is that very few if any UM seniors did as well as you. I\u2019m sure at least some psych students, at least after 2 years in school, did as well. But note that you came fairly close to a perfect score.\u201d<br \/>\nNevertheless, I did not feel that reading\u00a0Mindware\u00a0and taking the Coursera course had necessarily rid me of my biases. For one thing, I hadn\u2019t been tested beforehand, so I might just be a comparatively unbiased guy. For another, many of the test questions, including the one above, seemed somewhat remote from scenarios one might encounter in day-to-day life. They seemed to be \u201chard\u201d problems, not unlike the one about Linda the bank teller. Further, I had been, as Kahneman would say, \u201ccued.\u201d In contrast to the Michigan seniors, I knew exactly why I was being asked these questions, and approached them accordingly.<br \/>\nFor his part, Nisbett insisted that the results were meaningful. \u201cIf you\u2019re doing better in a testing context,\u201d he told me, \u201cyou\u2019ll jolly well be doing better in the real world.\u201d<\/p>\n<p>Nisbett\u2019s coursera course\u00a0and Hal Hershfield\u2019s close encounters with one\u2019s older self are hardly the only de-biasing methods out there. The New York\u2013based NeuroLeadership Institute offers organizations and individuals a variety of training sessions, webinars, and conferences that promise, among other things, to use brain science to teach participants to counter bias. This year\u2019s two-day summit will be held in New York next month; for $2,845, you could learn, for example, \u201cwhy are our brains so bad at thinking about the future, and how do we do it better?\u201d<\/p>\n<p>Philip E. Tetlock, a professor at the University of Pennsylvania\u2019s Wharton School, and his wife and research partner, Barbara Mellers, have for years been studying what they call \u201csuperforecasters\u201d: people who manage to sidestep cognitive biases and predict future events with far more accuracy than the pundits and so-called experts who show up on TV. In Tetlock\u2019s book\u00a0Superforecasting: The Art and Science of Prediction\u00a0(co-written with Dan Gardner), and in the commercial venture he and Mellers co-founded,\u00a0Good Judgment, they share the superforecasters\u2019 secret sauce.<br \/>\nOne of the most important ingredients is what Tetlock calls \u201cthe outside view.\u201d The inside view is a product of fundamental attribution error, base-rate neglect, and other biases that are constantly cajoling us into resting our judgments and predictions on good or vivid stories instead of on data and statistics. Tetlock explains, \u201cAt a wedding, someone sidles up to you and says, \u2018How long do you give them?\u2019 If you\u2019re shocked because you\u2019ve seen the devotion they show each other, you\u2019ve been sucked into the inside view.\u201d Something like 40 percent of marriages end in divorce, and that statistic is far more predictive of the fate of any particular marriage than a mutually adoring gaze. Not that you want to share that insight at the reception.<\/p>\n<p>The recent de-biasing interventions that scholars in the field have deemed the most promising are a handful of video games. Their genesis was in the Iraq War and the catastrophic weapons-of-mass-destruction blunder that led to it, which left the intelligence community reeling. In 2006, seeking to prevent another mistake of that magnitude, the U.S. government created the Intelligence Advanced Research Projects Activity (iarpa), an agency designed to use cutting-edge research and technology to improve intelligence-gathering and analysis. In 2011,\u00a0iarpa\u00a0initiated a program,\u00a0Sirius, to fund the development of \u201cserious\u201d video games that could combat or mitigate what were deemed to be the six most damaging biases: confirmation bias, fundamental attribution error, the bias blind spot (the feeling that one is less biased than the average person), the anchoring effect, the representativeness heuristic, and projection bias (the assumption that everybody else\u2019s thinking is the same as one\u2019s own).<br \/>\nConfirmation bias\u2014probably the most pervasive and damaging bias of them all\u2014leads us to look for evidence that confirms what we already think.<br \/>\nSix teams set out to develop such games, and two of them completed the process. The team that has gotten the most attention was led by Carey K. Morewedge, now a professor at Boston University. Together with collaborators who included staff from Creative Technologies, a company specializing in games and other simulations, and Leidos, a defense, intelligence, and health research company that does a lot of government work, Morewedge devised Missing. Some subjects played the game, which takes about three hours to complete, while others watched a video about cognitive bias. All were tested on bias-mitigation skills before the training, immediately afterward, and then finally after eight to 12 weeks had passed.<br \/>\nAfter taking the test, I played the game, which has the production value of a late-2000s PlayStation 3 first-person offering, with large-chested women and men, all of whom wear form-fitting clothes and navigate the landscape a bit tentatively. The player adopts the persona of a neighbor of a woman named Terry Hughes, who, in the first part of the game, has mysteriously gone missing. In the second, she has reemerged and needs your help to look into some skulduggery at her company. Along the way, you\u2019re asked to make judgments and predictions\u2014some having to do with the story and some about unrelated issues\u2014which are designed to call your biases into play. You\u2019re given immediate feedback on your answers.<br \/>\nFor example, as you\u2019re searching Terry\u2019s apartment, the building superintendent knocks on the door and asks you, apropos of nothing, about Mary, another tenant, whom he describes as \u201cnot a jock.\u201d He says 70 percent of the tenants go to Rocky\u2019s Gym, 10 percent go to Entropy Fitness, and 20 percent just stay at home and watch Netflix. Which gym, he asks, do you think Mary probably goes to? A wrong answer, reached thanks to base-rate neglect (a form of the representativeness heuristic) is \u201cNone. Mary is a couch potato.\u201d The right answer\u2014based on the data the super has helpfully provided\u2014is Rocky\u2019s Gym. When the participants in the study were tested immediately after playing the game or watching the video and then a couple of months later, everybody improved, but the game players improved more than the video watchers.<\/p>\n<p>When I spoke with Morewedge, he said he saw the results as supporting the research and insights of Richard Nisbett. \u201cNisbett\u2019s work was largely written off by the field, the assumption being that training can\u2019t reduce bias,\u201d he told me. \u201cThe literature on training suggests books and classes are fine entertainment but largely ineffectual. But the game has very large effects. It surprised everyone.\u201d<\/p>\n<p>I took the test again soon after playing the game, with mixed results. I showed notable improvement in confirmation bias, fundamental attribution error, and the representativeness heuristic, and improved slightly in bias blind spot and anchoring bias. My lowest initial score\u201444.8 percent\u2014was in projection bias. It actually dropped a bit after I played the game. (I really need to stop assuming that everybody thinks like me.) But even the positive results reminded me of something Daniel Kahneman had told me. \u201cPencil-and-paper doesn\u2019t convince me,\u201d he said. \u201cA test can be given even a couple of years later. But the test cues the test-taker. It reminds him what it\u2019s all about.\u201d<\/p>\n<p>I had taken Nisbett\u2019s and Morewedge\u2019s tests on a computer screen, not on paper, but the point remains. It\u2019s one thing for the effects of training to show up in the form of improved results on a test\u2014when you\u2019re on your guard, maybe even looking for tricks\u2014and quite another for the effects to show up in the form of real-life behavior. Morewedge told me that some tentative real-world scenarios along the lines of Missing have shown \u201cpromising results,\u201d but that it\u2019s too soon to talk about them.<\/p>\n<p>I am neither as much\u00a0of a pessimist as Daniel Kahneman nor as much of an optimist as Richard Nisbett. Since immersing myself in the field, I have noticed a few changes in my behavior. For example, one hot day recently, I decided to buy a bottle of water in a vending machine for $2. The bottle didn\u2019t come out; upon inspection, I realized that the mechanism holding the bottle in place was broken. However, right next to it was another row of water bottles, and clearly the mechanism in that row was in order. My instinct was to not buy a bottle from the \u201cgood\u201d row, because $4 for a bottle of water is too much. But all of my training in cognitive biases told me that was faulty thinking. I would be spending $2 for the water\u2014a price I was willing to pay,\u00a0as had already been established. So I put the money in and got the water, which I happily drank.<\/p>\n<p>In the future, I will monitor my thoughts and reactions as best I can. Let\u2019s say I\u2019m looking to hire a research assistant. Candidate A has sterling references and experience but appears tongue-tied and can\u2019t look me in the eye; Candidate B loves to talk NBA basketball\u2014my favorite topic!\u2014but his recommendations are mediocre at best. Will I have what it takes to overcome fundamental attribution error and hire Candidate A?<br \/>\nOr let\u2019s say there is an officeholder I despise for reasons of temperament, behavior, and ideology. And let\u2019s further say that under this person\u2019s administration, the national economy is performing well. Will I be able to dislodge my powerful confirmation bias and allow the possibility that the person deserves some credit?<br \/>\nAs for the matter that Hal Hershfield brought up in the first place\u2014estate planning\u2014I have always been the proverbial ant, storing up my food for winter while the grasshoppers sing and play. In other words, I have always maxed out contributions to 401(k)s, Roth IRAs, Simplified Employee Pensions, 403(b)s, 457(b)s, and pretty much every alphabet-soup savings choice presented to me. But as good a saver as I am, I am that bad a procrastinator. Months ago, my financial adviser offered to evaluate, for free, my will, which was put together a couple of decades ago and surely needs revising. There\u2019s something about drawing up a will that creates a perfect storm of biases, from the ambiguity effect (\u201cthe tendency to avoid options for which missing information makes the probability seem \u2018unknown,\u2019\u2009\u201d as Wikipedia defines it) to normalcy bias (\u201cthe refusal to plan for, or react to, a disaster which has never happened before\u201d), all of them culminating in the ostrich effect (do I really need to explain?). My adviser sent me a prepaid FedEx envelope, which has been lying on the floor of my office gathering dust. It is still there. As hindsight bias tells me, I knew that would happen.<\/p>\n<p>BEN YAGODA\u2019s books include\u00a0The B-Side: The Death of Tin Pan Alley and the Rebirth of the Great American Song\u00a0and\u00a0About Town:\u00a0The New Yorker\u00a0and the World It Made.<\/p>\n","protected":false},"excerpt":{"rendered":"<p>The Cognitive Biases Tricking Your Brain Science suggests we\u2019re hardwired to delude ourselves. Can we do anything about it? By\u00a0Ben Yagoda Atlantic September 2019 I am staring\u00a0at a photograph of myself that shows me 20 years older than I am &hellip; <a href=\"https:\/\/www.ronperrier.net\/2020\/01\/05\/your-lying-mind\/\">Continue reading <span class=\"meta-nav\">&rarr;<\/span><\/a><\/p>\n","protected":false},"author":1,"featured_media":0,"comment_status":"closed","ping_status":"closed","sticky":false,"template":"","format":"standard","meta":{"jetpack_post_was_ever_published":false,"_jetpack_newsletter_access":"","_jetpack_dont_email_post_to_subs":false,"_jetpack_newsletter_tier_id":0,"_jetpack_memberships_contains_paywalled_content":false,"_jetpack_memberships_contains_paid_content":false,"footnotes":"","jetpack_publicize_message":"","jetpack_publicize_feature_enabled":true,"jetpack_social_post_already_shared":false,"jetpack_social_options":{"image_generator_settings":{"template":"highway","default_image_id":0,"enabled":false},"version":2}},"categories":[1],"tags":[],"class_list":["post-12458","post","type-post","status-publish","format-standard","hentry","category-uncategorized"],"aioseo_notices":[],"jetpack_publicize_connections":[],"jetpack_featured_media_url":"","jetpack_sharing_enabled":true,"jetpack_shortlink":"https:\/\/wp.me\/p2Ncip-3eW","_links":{"self":[{"href":"https:\/\/www.ronperrier.net\/wp-json\/wp\/v2\/posts\/12458","targetHints":{"allow":["GET"]}}],"collection":[{"href":"https:\/\/www.ronperrier.net\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/www.ronperrier.net\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/www.ronperrier.net\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/www.ronperrier.net\/wp-json\/wp\/v2\/comments?post=12458"}],"version-history":[{"count":1,"href":"https:\/\/www.ronperrier.net\/wp-json\/wp\/v2\/posts\/12458\/revisions"}],"predecessor-version":[{"id":12459,"href":"https:\/\/www.ronperrier.net\/wp-json\/wp\/v2\/posts\/12458\/revisions\/12459"}],"wp:attachment":[{"href":"https:\/\/www.ronperrier.net\/wp-json\/wp\/v2\/media?parent=12458"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/www.ronperrier.net\/wp-json\/wp\/v2\/categories?post=12458"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/www.ronperrier.net\/wp-json\/wp\/v2\/tags?post=12458"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}